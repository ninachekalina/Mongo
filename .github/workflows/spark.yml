name: Spark CI

on:
  push:
    branches:
      - main
    paths:
      - '.github/workflows/spark.yml'
  pull_request:
    branches:
      - main
    paths:
      - '.github/workflows/spark.yml'
  workflow_dispatch:
    inputs:
      query_key:
        description: 'Key to extract query from artifact'
        required: false

jobs:
  spark-submit:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Setup Spark
        uses: vemonet/setup-spark@v1
        with:
          spark-version: '3.5.3'
          hadoop-version: '3'

      - name: Check Spark version
        run: spark-submit --version

      - name: Install Kaggle CLI
        run: |
          pip install kaggle

      - name: Download Netflix dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          mkdir -p data
          kaggle datasets download -d chekalinanina/netflix-titles -p data --unzip
          ls -lah data/

      - name: Check downloaded files
        run: ls -lah data/

      # Prepare the output file and write each query result with a unique key
      - name: Prepare Spark result file
        run: |
          OUTPUT_FILE="/tmp/spark_output.json"
          echo "{" > $OUTPUT_FILE  # Start the JSON object
          FIRST=true  # Flag for the first entry

          function run_query {
            QUERY=$1
            KEY=$2
            echo "Running query for $KEY..."
            RESULT=$(spark-submit /tmp/spark_query.py | tail -n +2 | jq -R . | paste -sd "," -)  # Extracting the result

            if [ -n "$RESULT" ]; then  # If results are not empty
              if [ "$FIRST" = true ]; then
                FIRST=false
              else
                echo "," >> $OUTPUT_FILE  # Add a comma between entries
              fi
              echo "\"$KEY\": [$RESULT]" >> $OUTPUT_FILE  # Write results with the key
            fi
          }

          # Define the queries and their keys
          echo "from pyspark.sql import SparkSession" > /tmp/spark_query.py
          echo "spark = SparkSession.builder.appName('Netflix').getOrCreate()" >> /tmp/spark_query.py
          echo "df = spark.read.csv('/home/runner/work/Mongo/Mongo/data/netflix_titles.csv', header=True, inferSchema=True)" >> /tmp/spark_query.py

          # Example queries with their unique keys
          echo "df.select('listed_in').distinct().show()" >> /tmp/spark_query.py
          run_query "df.select('listed_in').distinct().collect()" "unique_genres"

          echo "df.filter(df['release_year'] > 2015).show()" >> /tmp/spark_query.py
          run_query "df.filter(df['release_year'] > 2015).collect()" "movies_after_2015"

          echo "df.orderBy(df['duration'], ascending=False).limit(10).show()" >> /tmp/spark_query.py
          run_query "df.orderBy(df['duration'], ascending=False).limit(10).collect()" "top_10_longest_movies"

          echo "df.groupBy('type').count().show()" >> /tmp/spark_query.py
          run_query "df.groupBy('type').count().collect()" "movie_count_by_type"

          # Add more queries here with unique keys...

          echo "}" >> $OUTPUT_FILE  # Close the JSON object

      # Upload the results as an artifact
      - name: Upload Spark results as JSON
        uses: actions/upload-artifact@v4
        with:
          name: spark-output
          path: /tmp/spark_output.json
