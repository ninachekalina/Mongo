name: Spark CI

on:
  push:
    branches:
      - main
    paths:
      - '.github/workflows/spark.yml'
      - 'test_queries_spark.py'
  pull_request:
    branches:
      - main
    paths:
      - '.github/workflows/spark.yml'
      - 'test_queries_spark.py'
  workflow_dispatch:
    inputs:
      query_key:
        description: 'key to extract query from artifact'
        required: false

jobs:
  test-spark:
    runs-on: ubuntu-latest
    services:
      spark:
        image: bitnami/spark:latest  # Используем образ Spark от Bitnami
        ports:
          - 8080:8080
        env:
          SPARK_MODE: master
        options: >-
          --health-cmd "curl -f http://localhost:8080"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install jq and pip
        run: |
          sudo apt-get install -y jq
          sudo apt-get install -y python3-pip

      - name: Wait for Spark to be ready
        run: |
          echo "Waiting for Spark service..."
          sleep 10
          for i in {1..30}; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8080)
            if [[ "$STATUS" == "200" ]]; then
              echo "Spark is ready!"
              exit 0
            fi
            echo "Waiting for Spark to start ($i/30)..."
            sleep 5
          done
          echo "Spark did not start in time."
          exit 1

      - name: Debug Spark Logs
        run: docker logs $(docker ps -q --filter "ancestor=bitnami/spark:latest")

      - name: Download Netflix dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          pip install kaggle
          mkdir -p data
          kaggle datasets download -d chekalinanina/netflix-titles -p data --unzip
          ls -lah data/  # Проверка наличия скачанного файла

      - name: Run Spark job
        run: |
          docker exec -it $(docker ps -q --filter "ancestor=bitnami/spark:latest") spark-submit --master spark://spark:7077 test_queries_spark.py

      - name: Upload Spark results
        uses: actions/upload-artifact@v4
        with:
          name: spark-output
          path: output/results.json  # Укажите путь к вашему выходному файлу
