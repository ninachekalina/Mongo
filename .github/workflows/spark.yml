name: Spark CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      query_key:
        description: 'key to extract query from artifact'
        required: false

jobs:
  test-spark:
    runs-on: ubuntu-latest
    services:
      spark:
        image: bitnami/spark:latest
        ports:
          - 4040:4040  # Spark UI порт
          - 7077:7077  # Spark master порт
        options: >-
          --health-cmd "curl --silent --fail localhost:4040"
          --health-interval 20s
          --health-timeout 10s
          --health-retries 10
          --env SPARK_MODE=master
          --env SPARK_WORKER_MEMORY=2g
          --env SPARK_DRIVER_MEMORY=2g
          --env SPARK_EXECUTOR_MEMORY=2g
          --env SPARK_DRIVER_CORES=1
          --env SPARK_EXECUTOR_CORES=1
          --env SPARK_MASTER=local[*]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y openjdk-17-jdk wget jq python3-pip
          pip install pyspark kaggle

      - name: Download and Extract Spark
        run: |
          wget -q https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz
          tar -xzf spark-3.4.0-bin-hadoop3.tgz -C /opt/
          mv /opt/spark-3.4.0-bin-hadoop3 /opt/spark
          echo "SPARK_HOME=/opt/spark" >> $GITHUB_ENV
          echo "PATH=$SPARK_HOME/bin:$PATH" >> $GITHUB_ENV

      - name: Set environment variables
        run: |
          echo "export SPARK_MASTER=local[*]" >> $GITHUB_ENV
          echo "export PYSPARK_PYTHON=python3" >> $GITHUB_ENV

      - name: Check Spark version
        run: |
          spark-submit --version

      - name: Download dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          mkdir -p data
          kaggle datasets download -d chekalinanina/netflix-titles -p data --unzip
          ls -lah data/

      - name: Check downloaded files
        run: ls -lah data/

      - name: Upload dataset to Spark
        run: |
          SPARK_DIR="/opt/spark"
          spark-submit --class org.apache.spark.examples.SparkPi --master $SPARK_MASTER $SPARK_DIR/examples/jars/spark-examples_2.12-3.4.0.jar 1000

      - name: Run Spark operations
        run: |
          # Пример операции с PySpark
          echo "from pyspark.sql import SparkSession" > /tmp/spark_query.py
          echo "spark = SparkSession.builder.appName('Netflix').getOrCreate()" >> /tmp/spark_query.py
          echo "df = spark.read.csv('/data/netflix_titles.csv', header=True, inferSchema=True)" >> /tmp/spark_query.py
          echo "df.show()" >> /tmp/spark_query.py
          spark-submit /tmp/spark_query.py

      - name: Save Spark results to JSON
        run: |
          OUTPUT_FILE="/tmp/spark_output.json"
          echo "{" > $OUTPUT_FILE

          echo '"top_titles":' >> $OUTPUT_FILE
          spark-submit /tmp/spark_query.py | jq -R -s -c 'split("\n")[:-1]' >> $OUTPUT_FILE
          echo "," >> $OUTPUT_FILE

          echo '"average_duration":' >> $OUTPUT_FILE
          spark-submit /tmp/spark_query.py | jq -R -s -c 'split("\n")[:-1]' >> $OUTPUT_FILE
          echo "," >> $OUTPUT_FILE

          echo '"release_year_count":' >> $OUTPUT_FILE
          spark-submit /tmp/spark_query.py | jq -R -s -c 'split("\n")[:-1]' >> $OUTPUT_FILE
          echo "," >> $OUTPUT_FILE

          echo "}" >> $OUTPUT_FILE

      - name: Upload Spark results as JSON
        uses: actions/upload-artifact@v4
        with:
          name: spark-output
          path: /tmp/spark_output.json
