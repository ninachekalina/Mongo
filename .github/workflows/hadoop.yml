name: Hadoop CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:

jobs:
  test-hadoop:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y openjdk-17-jdk ssh unzip wget

      - name: Download and extract Hadoop
        run: |
          wget -nc -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
          tar -xzf hadoop-3.3.6.tar.gz -C /opt/
          mv /opt/hadoop-3.3.6 /opt/hadoop
          echo "HADOOP_HOME=/opt/hadoop" >> $GITHUB_ENV
          echo "PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH" >> $GITHUB_ENV

      - name: Configure SSH for Hadoop
        run: |
          ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
          cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
          chmod 600 ~/.ssh/authorized_keys
          echo "StrictHostKeyChecking no" >> ~/.ssh/config
          echo "UserKnownHostsFile=/dev/null" >> ~/.ssh/config

      - name: Configure Hadoop
        run: |
          cat <<EOF > /opt/hadoop/etc/hadoop/core-site.xml
          <configuration>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://localhost:9000</value>
            </property>
          </configuration>
          EOF
          cat <<EOF > /opt/hadoop/etc/hadoop/hdfs-site.xml
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
          </configuration>
          EOF

      - name: Format NameNode
        run: /opt/hadoop/bin/hdfs namenode -format

      - name: Start Hadoop
        run: |
          /opt/hadoop/sbin/start-dfs.sh
          sleep 5
          jps

      - name: Create HDFS directories
        run: |
          /opt/hadoop/bin/hdfs dfs -mkdir -p /user/runner
          /opt/hadoop/bin/hdfs dfs -mkdir -p /data
          /opt/hadoop/bin/hdfs dfs -chmod 777 /data
          /opt/hadoop/bin/hdfs dfs -mkdir -p /data/backup/
          /opt/hadoop/bin/hdfs dfs -mkdir -p /data/moved/

      - name: Download Russian Demography dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          pip install kaggle
          mkdir -p data
          kaggle datasets download -d chekalinanina/russian-demography -p data --unzip
          ls -lah data/

      - name: Upload dataset to HDFS
        run: |
          /opt/hadoop/bin/hdfs dfs -put data/* /data/
          /opt/hadoop/bin/hdfs dfs -ls /data/

      - name: Run all HDFS operations and save JSON
        run: |
          cat <<EOF > hdfs_output.json
          {
            "size": "$( /opt/hadoop/bin/hdfs dfs -du -h /data/ | sed 's/"/\\"/g' )",
            "head": "$( for file in $(/opt/hadoop/bin/hdfs dfs -ls /data/ | awk '{print \$8}'); do /opt/hadoop/bin/hdfs dfs -cat \$file | head -n 5; done | sed 's/"/\\"/g' )",
            "move": "$( /opt/hadoop/bin/hdfs dfs -mv /data/*.csv /data/moved/ 2>&1 && /opt/hadoop/bin/hdfs dfs -ls /data/moved/ | sed 's/"/\\"/g' )",
            "grep": "$( /opt/hadoop/bin/hdfs dfs -cat /data/moved/*.csv | grep -i "birth" | head -n 10 | sed 's/"/\\"/g' )",
            "replication": "$( /opt/hadoop/bin/hdfs dfs -stat %r /data/moved/*.csv 2>/dev/null | sed 's/"/\\"/g' )",
            "delete": "$( /opt/hadoop/bin/hdfs dfs -rm -r /data/backup/ 2>&1 && /opt/hadoop/bin/hdfs dfs -ls /data/ | sed 's/"/\\"/g' )"
          }
          EOF
          cat hdfs_output.json

      - name: Upload HDFS JSON log as artifact
        uses: actions/upload-artifact@v4
        with:
          name: hadoop-hdfs-logs
          path: hdfs_output.json


     
 
