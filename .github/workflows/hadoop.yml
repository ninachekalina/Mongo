name: Hadoop CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      command_key:
        description: 'Command Key (size, head, move, grep, replication, delete)'
        required: true
        default: 'size'

jobs:
  test-hadoop:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y openjdk-17-jdk ssh unzip wget

      - name: Download and extract Hadoop
        run: |
          wget -nc -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
          tar -xzf hadoop-3.3.6.tar.gz -C /opt/
          mv /opt/hadoop-3.3.6 /opt/hadoop
          echo "HADOOP_HOME=/opt/hadoop" >> $GITHUB_ENV
          echo "PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH" >> $GITHUB_ENV

      - name: Configure SSH for Hadoop
        run: |
          ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
          cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
          chmod 600 ~/.ssh/authorized_keys
          echo "StrictHostKeyChecking no" >> ~/.ssh/config
          echo "UserKnownHostsFile=/dev/null" >> ~/.ssh/config

      - name: Configure Hadoop
        run: |
          cat <<EOF > /opt/hadoop/etc/hadoop/core-site.xml
          <configuration>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://localhost:9000</value>
            </property>
          </configuration>
          EOF
          cat <<EOF > /opt/hadoop/etc/hadoop/hdfs-site.xml
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
          </configuration>
          EOF

      - name: Format NameNode
        run: /opt/hadoop/bin/hdfs namenode -format

      - name: Start Hadoop
        run: |
          /opt/hadoop/sbin/start-dfs.sh
          sleep 5
          jps  

      - name: Create HDFS directories
        run: |
          /opt/hadoop/bin/hdfs dfs -mkdir -p /user/runner
          /opt/hadoop/bin/hdfs dfs -mkdir -p /data
          /opt/hadoop/bin/hdfs dfs -chmod 777 /data
          /opt/hadoop/bin/hdfs dfs -mkdir -p /data/backup/
          /opt/hadoop/bin/hdfs dfs -mkdir -p /data/moved/

      - name: Download Russian Demography dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          pip install kaggle
          mkdir -p data
          kaggle datasets download -d chekalinanina/russian-demography -p data --unzip
          ls -lah data/

      - name: Upload dataset to HDFS
        run: |
          /opt/hadoop/bin/hdfs dfs -put data/* /data/
          /opt/hadoop/bin/hdfs dfs -ls /data/ > hdfs_log.txt

      - name: Run HDFS operation based on input key
        run: |
          case "${{ github.event.inputs.command_key }}" in
            "size")
              echo "Checking dataset size in HDFS" >> hdfs_log.txt
              /opt/hadoop/bin/hdfs dfs -du -h /data/ >> hdfs_log.txt
              ;;
            "head")
              echo "Displaying first few lines of CSV files" >> hdfs_log.txt
              for file in $(/opt/hadoop/bin/hdfs dfs -ls /data/ | awk '{print $8}'); do
                /opt/hadoop/bin/hdfs dfs -cat $file | head -n 5 >> hdfs_log.txt
              done
              ;;
            "move")
              echo "Move dataset within HDFS" >> hdfs_log.txt
              /opt/hadoop/bin/hdfs dfs -mv /data/*.csv /data/moved/
              /opt/hadoop/bin/hdfs dfs -ls /data/moved/ >> hdfs_log.txt
              ;;
            "grep")
              echo "Search for keyword 'birth' in dataset" >> hdfs_log.txt
              /opt/hadoop/bin/hdfs dfs -cat /data/moved/*.csv | grep -i "birth" | head -n 10 >> hdfs_log.txt
              ;;
            "replication")
              echo "Check file replication" >> hdfs_log.txt
              /opt/hadoop/bin/hdfs dfs -stat %r /data/moved/*.csv >> hdfs_log.txt
              ;;
            "delete")
              echo "Remove backup dataset" >> hdfs_log.txt
              /opt/hadoop/bin/hdfs dfs -rm -r /data/backup/
              /opt/hadoop/bin/hdfs dfs -ls /data/ >> hdfs_log.txt
              ;;
            *)
              echo "Invalid command key: ${GITHUB_EVENT_INPUTS_COMMAND_KEY}" >> hdfs_log.txt
              ;;
          esac

      - name: Save Hadoop HDFS logs
        uses: actions/upload-artifact@v4
        with:
          name: hadoop-hdfs-logs
          path: hdfs_log.txt
