name: Hadoop CI

on:
  push:
    branches:
      - main
    paths:
      - '.github/workflows/hadoop.yml'  # Указываем путь к файлу
  pull_request:
    branches:
      - main
    paths:
      - '.github/workflows/hadoop.yml'  # Указываем путь к файлу
  workflow_dispatch:

jobs:
  test-hadoop:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y openjdk-17-jdk ssh unzip wget jq python3-pip
          pip install kaggle

      - name: Download and extract Hadoop
        run: |
          wget -nc -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
          tar -xzf hadoop-3.3.6.tar.gz -C /opt/
          mv /opt/hadoop-3.3.6 /opt/hadoop
          echo "HADOOP_HOME=/opt/hadoop" >> $GITHUB_ENV
          echo "PATH=/opt/hadoop/bin:/opt/hadoop/sbin:$PATH" >> $GITHUB_ENV

      - name: Configure SSH for Hadoop
        run: |
          ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
          cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
          chmod 600 ~/.ssh/authorized_keys
          echo "StrictHostKeyChecking no" >> ~/.ssh/config
          echo "UserKnownHostsFile=/dev/null" >> ~/.ssh/config

      - name: Configure Hadoop
        run: |
          cat <<EOF > /opt/hadoop/etc/hadoop/core-site.xml
          <configuration>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://localhost:9000</value>
            </property>
          </configuration>
          EOF

          cat <<EOF > /opt/hadoop/etc/hadoop/hdfs-site.xml
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
          </configuration>
          EOF

      - name: Format NameNode
        run: hdfs namenode -format

      - name: Start Hadoop
        run: |
          start-dfs.sh
          sleep 5
          jps

      - name: Create HDFS directories
        run: |
          hdfs dfs -mkdir -p /user/runner
          hdfs dfs -mkdir -p /data
          hdfs dfs -chmod 777 /data
          hdfs dfs -mkdir -p /data/backup/
          hdfs dfs -mkdir -p /data/moved/

      - name: Download Russian Demography dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          mkdir -p data
          kaggle datasets download -d chekalinanina/russian-demography -p data --unzip
          ls -lah data/

      - name: Upload dataset to HDFS
        run: |
          hdfs dfs -put data/* /data/
          hdfs dfs -ls /data/

      - name: Save HDFS results to JSON
        run: |
          OUTPUT_FILE="/tmp/hdfs_output.json"
          echo "{" > $OUTPUT_FILE

          echo '"size":' >> $OUTPUT_FILE
          hdfs dfs -du -h /data | jq -R -s -c 'split("\n")[:-1]' >> $OUTPUT_FILE
          echo "," >> $OUTPUT_FILE

          echo '"head":' >> $OUTPUT_FILE
          hdfs dfs -cat /data/russian_demography.csv | head -n 5 | jq -R -s -c 'split("\n")[:-1]' >> $OUTPUT_FILE
          echo "," >> $OUTPUT_FILE

          echo '"move":' >> $OUTPUT_FILE
          hdfs dfs -ls /data/moved | jq -R -s -c 'split("\n")[:-1]' >> $OUTPUT_FILE
          echo "," >> $OUTPUT_FILE

          echo '"grep":' >> $OUTPUT_FILE
          hdfs dfs -cat /data/russian_demography.csv | grep -m 1 year | jq -R -c '.' >> $OUTPUT_FILE
          echo "," >> $OUTPUT_FILE

          echo '"replication":' >> $OUTPUT_FILE
          hdfs getconf -confKey dfs.replication | jq -R -c '.' >> $OUTPUT_FILE
          echo "," >> $OUTPUT_FILE

          echo '"delete":' >> $OUTPUT_FILE
          (hdfs dfs -rm -r -skipTrash /data/backup && hdfs dfs -ls /data) | jq -R -s -c 'split("\n")[:-1]' >> $OUTPUT_FILE

          echo "}" >> $OUTPUT_FILE

      - name: Upload HDFS JSON log as artifact
        uses: actions/upload-artifact@v4
        with:
          name: hadoop-hdfs-logs
          path: /tmp/hdfs_output.json
